{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2523e46",
   "metadata": {},
   "source": [
    "# GLoVe\n",
    "\n",
    "In this task we will implement the GLoVe algorithm for generating word embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49b6d5ac",
   "metadata": {},
   "source": [
    "We will Game of Thrones dialogue from all seasons as our corpus. Each line consists of a dialogue spoken by a character in a scene."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d953ce5",
   "metadata": {},
   "source": [
    "1) Given the corpus, define a function that removes all the punctuations and stop words from the text. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "caa1661a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e50f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(corpus):\n",
    "    s_words = stopwords.words('english')\n",
    "    puncts = string.punctuation\n",
    "\n",
    "    n_sentences = []\n",
    "\n",
    "    for sent in corpus:\n",
    "        n_sent = [w.lower() for w in sent if w not in puncts and w.lower() not in s_words]\n",
    "        n_sentences.append(n_sent)\n",
    "\n",
    "    return n_sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ece34f46",
   "metadata": {},
   "source": [
    "2) From normalized sentences obtained in the previous step, create word-word frequency matrix with all the unique words. You will also need to create a word2index mapping (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2a144847",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {} # you can replace this with a more efficient data structure\n",
    "\n",
    "def generate_frequency_matrix(corpus, window_size=3):\n",
    "    # write your code snippet here\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2f347d9",
   "metadata": {},
   "source": [
    "3) Define weighting function used in GLoVe. (4 points)\n",
    "\n",
    "$f(x) = (\\frac{x}{x_{max}})^\\alpha$ if $x < x_{max}$, 1 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9379e3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting_func(x, x_max=100):\n",
    "    # write your code snippet here\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ab98f28",
   "metadata": {},
   "source": [
    "4) Create the Glove model class using pytorch. (10 points)\n",
    "   \n",
    "   Hints: \n",
    "   1. The forward pass will compute $W_i^T \\hat{W_j} + b_i + \\hat{b}_j$\n",
    "   2. $W, \\hat{W}, b_i, \\hat{b}_j$ will be the parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a18228a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "532b37be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove(nn.Module):\n",
    "    # write your code snippet here\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8c6f188",
   "metadata": {},
   "source": [
    "5) Write a function which trains the model on the frequency matrix. Ignore the 0 entries in the matrix. (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8939e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since many entries in the matrix would be 0, it makes sense to explicitly keep track of the positive entries and iterate\n",
    "# over them rather than writing a nested for loop...\n",
    "# You can wrap this entries in a torch Dataset class\n",
    "####################### optional ####################\n",
    "class GOT_data(Dataset):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self):\n",
    "        pass\n",
    "\n",
    "#####################################################\n",
    "\n",
    "# Adopt your code to incorporate mini-batch training\n",
    "def train(model, data, epochs=5, learning_rate=0.001):\n",
    "    # write your code snippet here\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7baf2bb1",
   "metadata": {},
   "source": [
    "6) Write a function to generate embedding of a given word. Note that the embeddings of a word ($i$) would be $W_i + \\hat{W}_i$ (5 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a3510b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ae55590",
   "metadata": {},
   "source": [
    "# Intrinsic evaluation of embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d5e816c",
   "metadata": {},
   "source": [
    "(Slide 47, lecture_4)\n",
    "Word similarity task is often used as an intrinsic evaluation criteria. In the dataset file you will find a list of word pairs with their similarity scores as judged by humans. The task would be to judge how well are the word vectors aligned to human judgement. We will use word2vec embedding vectors trained on the google news corpus. (Ignore the pairs where at least one the words is absent in the corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "74d74785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd7b0f90",
   "metadata": {},
   "source": [
    "7) Write a function which takes as input two words and computes the cosine similarity between them. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "08bb2576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word1, word2):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd3fdc12",
   "metadata": {},
   "source": [
    "8) Compute the similarity between all the word pairs in the list and sort them based on the similarity scores. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350fa9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72c8c139",
   "metadata": {},
   "source": [
    "9) Sort the word pairs in the list based on the human judgement scores. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54abef94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77379de5",
   "metadata": {},
   "source": [
    "10) Compute spearman rank correlation between the two ranked lists obtained in the previous two steps. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ff3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "760c4407",
   "metadata": {},
   "source": [
    "# Word embedding based classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dde45a84",
   "metadata": {},
   "source": [
    "We will design a simple sentiment classifier based on the pre-trained word embeddings (google news).\n",
    "\n",
    "Each data point is a movie review and the sentiment could be either positive (1) or negative (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ec07c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "22af215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentiment_test_X.p', 'rb') as fs:\n",
    "    test_X = pickle.load(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e4d133a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5f2b080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentiment_test_y.p', 'rb') as fs:\n",
    "    test_y = pickle.load(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4698698d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8464aa0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'you',\n",
       " 'sometimes',\n",
       " 'like',\n",
       " 'to',\n",
       " 'go',\n",
       " 'to',\n",
       " 'the',\n",
       " 'movies',\n",
       " 'to',\n",
       " 'have',\n",
       " 'fun',\n",
       " ',',\n",
       " 'Wasabi',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'place',\n",
       " 'to',\n",
       " 'start',\n",
       " '.']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e60c4dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0cbcef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentiment_train_X.p', 'rb') as fs:\n",
    "    train_X = pickle.load(fs)\n",
    "with open('sentiment_train_y.p', 'rb') as fs:\n",
    "    train_y = pickle.load(fs)\n",
    "with open('sentiment_val_X.p', 'rb') as fs:\n",
    "    val_X = pickle.load(fs)\n",
    "with open('sentiment_val_y.p', 'rb') as fs:\n",
    "    val_y = pickle.load(fs)        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b1bb687",
   "metadata": {},
   "source": [
    "11) Given a review, compute its embedding by averaging over the embedding of its constituent words. Define a function which given a review as a list of words, generates its embeddings by averaging over the constituent word embeddings. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "167afa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(review):\n",
    "    # return embedding\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f38ecbb",
   "metadata": {},
   "source": [
    "12) Create a feed-forward network class with pytorch. (Hyperparamter choice such as number of layers, hidden size is left to you) (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bc5ab8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15fa977e",
   "metadata": {},
   "source": [
    "13) Create a Dataset class for efficiently enumerating over the dataset. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "58f0aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sent_data(Dataset):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self):\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a72e6361",
   "metadata": {},
   "source": [
    "14) Write a train function to train model. At the end of each epoch compute the validation accuracy and save the model with the best validation accuracy. (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fcd996de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adopt your code to incorporate mini-batch training\n",
    "# Use cross-entropy as your loss function\n",
    "def train(model, train_data, val_data, epochs=5, learning_rate=0.001):\n",
    "    # write your code snippet here\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ed22d35",
   "metadata": {},
   "source": [
    "15) Evaluate the trained model on the test set and report the test accuracy. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d0120739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aa57fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
