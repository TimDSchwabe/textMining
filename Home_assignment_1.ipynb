{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2523e46",
   "metadata": {},
   "source": [
    "# GLoVe\n",
    "\n",
    "In this task we will implement the GLoVe algorithm for generating word embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49b6d5ac",
   "metadata": {},
   "source": [
    "We will Game of Thrones dialogue from all seasons as our corpus. Each line consists of a dialogue spoken by a character in a scene."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d953ce5",
   "metadata": {},
   "source": [
    "1) Given the corpus, define a function that removes all the punctuations and stop words from the text. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa1661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize\n",
    "import string\n",
    "from gensim import utils\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e50f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text_ex(corpus):\n",
    "    s_words = stopwords.words('english')\n",
    "    puncts = string.punctuation\n",
    "\n",
    "    n_sentences = []\n",
    "\n",
    "    for sent in corpus:\n",
    "        n_sent = [w.lower() for w in sent if w not in puncts and w.lower() not in s_words]\n",
    "        n_sentences.append(n_sent)\n",
    "\n",
    "    return n_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d4a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(corpus):\n",
    "    s_words = stopwords.words('english')\n",
    "    puncts = string.punctuation + \"â€¦\" # special '...' character contained in the csv file\n",
    "\n",
    "    sents = sent_tokenize(corpus)\n",
    "    for i in range(len(sents)): # for one line in the corpus, containing multiple sentences\n",
    "        sents[i] = sents[i].translate(str.maketrans('','',puncts))\n",
    "        split_sent = [word.lower() for word in sents[i].split() if word not in s_words]\n",
    "        sents[i] = ' '.join(split_sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b13132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        for line in open('GOT_dialogues.csv', encoding=\"utf8\"):               \n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2731d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = MyCorpus()\n",
    "n_sentences = []\n",
    "for line in corpus:\n",
    "    n_sentences.extend(normalize_text(line))\n",
    "print(n_sentences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ece34f46",
   "metadata": {},
   "source": [
    "2) From normalized sentences obtained in the previous step, create word-word frequency matrix with all the unique words. You will also need to create a word2index mapping (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a144847",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "unique_tokens = set()\n",
    "for sent in n_sentences:\n",
    "    words = sent.split(' ')\n",
    "    unique_tokens.update(words)\n",
    "i = 0\n",
    "for w in unique_tokens:\n",
    "    word2index[w] = i\n",
    "    i += 1\n",
    "\n",
    "def generate_frequency_matrix(corpus: list[str], window_size=3):\n",
    "    freq_mat = np.zeros((len(word2index), len(word2index)), dtype=np.float32)\n",
    "    for sent in corpus: # iterate over the normalized sentences\n",
    "        words_in_sent = sent.split(' ')\n",
    "        for i in range(len(words_in_sent)): # iterate over each word in the sentence being the centre word\n",
    "            w_i = word2index[words_in_sent[i]] # row index of centre word in freq_mat\n",
    "            for j in range(1, window_size+1):\n",
    "                if i-j >= 0: # to its left\n",
    "                    w_j = word2index[words_in_sent[i-j]]\n",
    "                    freq_mat[w_i][w_j]+=1\n",
    "                if i+j < len(words_in_sent): # to its right\n",
    "                    w_j = word2index[words_in_sent[i+j]]\n",
    "                    freq_mat[w_i][w_j]+=1\n",
    "    return freq_mat\n",
    "\n",
    "freq_mat = generate_frequency_matrix(n_sentences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2f347d9",
   "metadata": {},
   "source": [
    "3) Define weighting function used in GLoVe. (4 points)\n",
    "\n",
    "$f(x) = (\\frac{x}{x_{max}})^\\alpha$ if $x < x_{max}$, 1 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9379e3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting_func(x, x_max=100):\n",
    "    # write your code snippet here\n",
    "    alpha = .75 # see https://aclanthology.org/D14-1162.pdf, p.4\n",
    "    return (x/x_max)**alpha if x < x_max else 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ab98f28",
   "metadata": {},
   "source": [
    "4) Create the Glove model class using pytorch. (10 points)\n",
    "   \n",
    "   Hints: \n",
    "   1. The forward pass will compute $W_i^T \\hat{W_j} + b_i + \\hat{b}_j$\n",
    "   2. $W, \\hat{W}, b_i, \\hat{b}_j$ will be the parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18228a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove(nn.Module):\n",
    "    def __init__(self, v_size, e_size, x_max=100):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.rand(v_size, e_size))\n",
    "        self.w_hat = nn.Parameter(torch.rand(v_size, e_size))\n",
    "\n",
    "        self.b = nn.Parameter(torch.rand(v_size))\n",
    "        self.b_hat = nn.Parameter(torch.rand(v_size))\n",
    "\n",
    "        self.weighting_func = lambda x : weighting_func(x, x_max)\n",
    "    \n",
    "    def forward(self, i, j, x):\n",
    "        out = torch.mul(torch.transpose(self.w[i]), self.w_hat[j])\n",
    "        out = (out + self.b[i] + self.b_hat[j] - np.log(freq_mat[i][j]))**2\n",
    "        out = torch.mul(self.weighting_func(x), out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab441f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Glove(len(unique_tokens), e_size=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8c6f188",
   "metadata": {},
   "source": [
    "5) Write a function which trains the model on the frequency matrix. Ignore the 0 entries in the matrix. (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8939e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since many entries in the matrix would be 0, it makes sense to explicitly keep track of the positive entries and iterate\n",
    "# over them rather than writing a nested for loop...\n",
    "# You can wrap this entries in a torch Dataset class\n",
    "####################### optional ####################\n",
    "class GOT_data(Dataset):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self):\n",
    "        pass\n",
    "\n",
    "#####################################################\n",
    "\n",
    "# Adopt your code to incorporate mini-batch training\n",
    "def train(model, data, epochs=5, learning_rate=0.001):\n",
    "    # write your code snippet here\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7baf2bb1",
   "metadata": {},
   "source": [
    "6) Write a function to generate embedding of a given word. Note that the embeddings of a word ($i$) would be $W_i + \\hat{W}_i$ (5 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a3510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(model, word):\n",
    "    return model.w[word2index[word]] + model.w_hat[word2index[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be60b33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_embedding(model, \"what\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ae55590",
   "metadata": {},
   "source": [
"# Intrinsic evaluation of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5e816c",
   "metadata": {},
   "source": [
    "(Slide 47, lecture_4)\n",
    "Word similarity task is often used as an intrinsic evaluation criteria. In the dataset file you will find a list of word pairs with their similarity scores as judged by humans. The task would be to judge how well are the word vectors aligned to human judgement. We will use word2vec embedding vectors trained on the google news corpus. (Ignore the pairs where at least one the words is absent in the corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d74785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7b0f90",
   "metadata": {},
   "source": [
    "7) Write a function which takes as input two words and computes the cosine similarity between them. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bb2576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word1, word2):\n",
    "    return wv.similarity(word1, word2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3fdc12",
   "metadata": {},
   "source": [
    "8) Compute the similarity between all the word pairs in the list and sort them based on the similarity scores. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350fa9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"wordsim353_dataset.csv\")\n",
    "df = df.sort_values(\"Human (mean)\")\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "similarities = []\n",
    "for idx, row in df.iterrows():\n",
    "    similarities.append(similarity(row[\"Word 1\"], row[\"Word 2\"]))\n",
    "df[\"Embedding\"] = similarities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c8c139",
   "metadata": {},
   "source": [
    "9) Sort the word pairs in the list based on the human judgement scores. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54abef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(\"Embedding\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77379de5",
   "metadata": {},
   "source": [
    "10) Compute spearman rank correlation between the two ranked lists obtained in the previous two steps. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ff3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "index = df.index.tolist()\n",
    "sorted_index = sorted(index)\n",
    "print(stats.spearmanr(index, sorted_index).statistic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c4407",
   "metadata": {},
   "source": [
    "# Word embedding based classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde45a84",
   "metadata": {},
   "source": [
    "We will design a simple sentiment classifier based on the pre-trained word embeddings (google news).\n",
    "\n",
    "Each data point is a movie review and the sentiment could be either positive (1) or negative (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec07c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22af215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentiment_test_X.p', 'rb') as fs:\n",
    "    test_X = pickle.load(fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d133a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2b080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentiment_test_y.p', 'rb') as fs:\n",
    "    test_y = pickle.load(fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4698698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8464aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c4dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbcef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentiment_train_X.p', 'rb') as fs:\n",
    "    train_X = pickle.load(fs)\n",
    "with open('sentiment_train_y.p', 'rb') as fs:\n",
    "    train_y = pickle.load(fs)\n",
    "with open('sentiment_val_X.p', 'rb') as fs:\n",
    "    val_X = pickle.load(fs)\n",
    "with open('sentiment_val_y.p', 'rb') as fs:\n",
    "    val_y = pickle.load(fs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1bb687",
   "metadata": {},
   "source": [
    "11) Given a review, compute its embedding by averaging over the embedding of its constituent words. Define a function which given a review as a list of words, generates its embeddings by averaging over the constituent word embeddings. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167afa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(review):\n",
    "    return wv.get_mean_vector(review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f38ecbb",
   "metadata": {},
   "source": [
    "12) Create a feed-forward network class with pytorch. (Hyperparamter choice such as number of layers, hidden size is left to you) (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ab8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(nn.Linear(300, 30), nn.Linear(30, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa977e",
   "metadata": {},
   "source": [
    "13) Create a Dataset class for efficiently enumerating over the dataset. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f0aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class sent_data(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = []\n",
    "        assert len(data) == len(labels)\n",
    "        for i, point in enumerate(data):\n",
    "            self.data.append(\n",
    "                (torch.tensor(generate_embedding(point)), torch.tensor(labels[i])))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e6361",
   "metadata": {},
   "source": [
    "14) Write a train function to train model. At the end of each epoch compute the validation accuracy and save the model with the best validation accuracy. (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd996de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adopt your code to incorporate mini-batch training\n",
    "# Use cross-entropy as your loss function\n",
    "\n",
    "\n",
    "def train(model, train_data, val_data, epochs=5, learning_rate=0.001):\n",
    "    # write your code snippet here\n",
    "    running_loss = 0.\n",
    "    best_acc = 0.\n",
    "    last_loss = 0.\n",
    "    best_model = model\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    train_loader = DataLoader(train_data, batch_size=1024, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=256)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_train_loss = 0\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "            loss = nn.BCEWithLogitsLoss()(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()*inputs.shape[0]\n",
    "        epoch_val_loss = 0\n",
    "        val_acc = 0\n",
    "        val_items = 0\n",
    "        correct_preds = 0\n",
    "        model.eval()\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            labels = labels.unsqueeze(1).float()\n",
    "            loss = nn.BCEWithLogitsLoss()(outputs, labels)\n",
    "            epoch_val_loss += loss.item()\n",
    "            correct_preds += (torch.round(nn.Sigmoid()\n",
    "                                          (outputs)) == labels).sum()\n",
    "            val_items += inputs.shape[0]\n",
    "        cur_acc = correct_preds/val_items\n",
    "        print(\n",
    "            f\"Epoch {epoch}\\ttrain loss\\t{epoch_train_loss/len(train_data):.4f}\\tval loss\\t{epoch_val_loss/val_items:.4f}\\tval acc\\t{cur_acc:.4f}\")\n",
    "        if cur_acc > best_acc:\n",
    "            best_acc = cur_acc\n",
    "            torch.save(model.state_dict(), \"model.torch\")\n",
    "            best_model = model\n",
    "    print(f\"saved model with val acc {best_acc:.4f}\")\n",
    "    return best_model\n",
    "\n",
    "\n",
    "model = train(Classifier(), sent_data(train_X, train_y),\n",
    "              sent_data(val_X, val_y), epochs=100, learning_rate=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed22d35",
   "metadata": {},
   "source": [
    "15) Evaluate the trained model on the test set and report the test accuracy. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0120739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "    model.eval()\n",
    "    correct_preds = 0\n",
    "    test_loader = DataLoader(test_data)\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        labels = labels.unsqueeze(1).float()\n",
    "        correct_preds += (torch.round(nn.Sigmoid()\n",
    "                                      (outputs)) == labels).sum()\n",
    "    acc = correct_preds/len(test_data)\n",
    "    print(f\"test acc\\t{acc:.4f}\")\n",
    "    return acc\n",
    "evaluate(model, sent_data(test_X, test_y))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
